############################################################
# ðŸ“˜ æ–‡ä»¶è¯´æ˜Žï¼šAIè§’è‰²å®šä¹‰
# æœ¬æ–‡ä»¶å®žçŽ°çš„åŠŸèƒ½ï¼šå®šä¹‰ä¸åŒAIè§’è‰²çš„èŒè´£å’Œæç¤ºè¯
#
# ðŸ“‹ ç¨‹åºæ•´ä½“ä¼ªä»£ç ï¼ˆä¸­æ–‡ï¼‰ï¼š
# 1. åˆå§‹åŒ–ä¾èµ–æ¨¡å—å’Œé…ç½®
# 2. å®šä¹‰æ ¸å¿ƒç±»å’Œå‡½æ•°
# 3. å®žçŽ°ä¸»è¦ä¸šåŠ¡é€»è¾‘
# 4. æä¾›å¯¹å¤–æŽ¥å£
# 5. å¼‚å¸¸å¤„ç†ä¸Žæ—¥å¿—è®°å½•
#
# ðŸ”„ ç¨‹åºæµç¨‹å›¾ï¼ˆé€»è¾‘æµï¼‰ï¼š
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚  è¾“å…¥æ•°æ®    â”‚
# â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
#        â†“
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚  æ ¸å¿ƒå¤„ç†é€»è¾‘ â”‚
# â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
#        â†“
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚  è¾“å‡ºç»“æžœ    â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# ðŸ“Š æ•°æ®ç®¡é“è¯´æ˜Žï¼š
# æ•°æ®æµå‘ï¼šè¾“å…¥æº â†’ æ•°æ®å¤„ç† â†’ æ ¸å¿ƒç®—æ³• â†’ è¾“å‡ºç›®æ ‡
#
# ðŸ§© æ–‡ä»¶ç»“æž„ï¼š
# - ç±»: AIRole, RoleConfig, AIResponse
# - å‡½æ•°: get_role_config, call_ai, call_ai_async, reason, write
#
# ðŸ”— ä¸»è¦ä¾èµ–ï¼š__future__, cloud, dataclasses, enum, json, orchestration, time, typing
#
# ðŸ•’ åˆ›å»ºæ—¶é—´ï¼š2025-12-18
############################################################

"""
AI è§’è‰²è°ƒç”¨è§„èŒƒ
================
æ ¹æ®è®¡åˆ’ä¹¦è®¾è®¡çš„ç»Ÿä¸€ AI è°ƒç”¨æŽ¥å£ï¼Œå°†"è°ƒç”¨æŸä¸ªæ¨¡åž‹"æŠ½è±¡æˆ"è°ƒç”¨æŸä¸ªè§’è‰²"ã€‚
æ¯ä¸ªè§’è‰²å°è£…äº†ä»»åŠ¡ç±»åž‹ã€ä¼˜å…ˆçº§ä»¥åŠè¿”å›žæ ¼å¼ï¼Œå¹¶äº¤ç”± ApiManager è‡ªè¡Œé€‰æ‹©æœ€ä½³ç«¯ç‚¹ã€‚

è§’è‰²åˆ—è¡¨ï¼š
- ai_reasoning: å¤æ‚æŽ¨ç†ã€é£Žé™©è¯„ä¼°ã€ç­–ç•¥å®¡æ ¸
- ai_writer: æ’°å†™æŠ¥å‘Šã€æ—¥è®°ã€é‚®ä»¶
- ai_memory: é•¿ä¸Šä¸‹æ–‡æ‘˜è¦ã€åŽ†å²å›žé¡¾
- ai_research: è”ç½‘æœç´¢å’Œæ‘˜è¦æ–°é—»
- ai_critic: æŒ‘åˆºå’Œç”Ÿæˆåä¾‹ã€å®¡æŸ¥äº¤æ˜“è®¡åˆ’
"""

from __future__ import annotations

import json
import time
from dataclasses import dataclass, field
from typing import Any, Callable, Dict, List, Optional, Union
from enum import Enum

from cloud.api_manager import ApiManager, ApiEndpoint, get_api_manager
from utils.smart_logger import get_logger

logger = get_logger("ai_roles")


class AIRole(str, Enum):
    """AI è§’è‰²æžšä¸¾"""
    REASONING = "ai_reasoning"    # å¤æ‚æŽ¨ç†ã€é£Žé™©è¯„ä¼°ã€ç­–ç•¥å®¡æ ¸
    WRITER = "ai_writer"          # æ’°å†™æŠ¥å‘Šã€æ—¥è®°ã€é‚®ä»¶
    MEMORY = "ai_memory"          # é•¿ä¸Šä¸‹æ–‡æ‘˜è¦ã€åŽ†å²å›žé¡¾
    RESEARCH = "ai_research"      # è”ç½‘æœç´¢å’Œæ‘˜è¦æ–°é—»
    CRITIC = "ai_critic"          # æŒ‘åˆºå’Œç”Ÿæˆåä¾‹


@dataclass
class RoleConfig:
    """è§’è‰²é…ç½®"""
    role: AIRole
    description: str
    preferred_endpoints: List[str]  # ä¼˜å…ˆä½¿ç”¨çš„ç«¯ç‚¹åç§°
    system_prompt: str
    default_max_tokens: int = 1024
    default_temperature: float = 0.7
    output_format: str = "text"  # text, json, markdown


@dataclass
class AIResponse:
    """AI è°ƒç”¨å“åº”"""
    content: str
    endpoint: str
    role: str
    latency: float
    raw: Optional[Dict] = None
    parsed: Optional[Dict] = None
    success: bool = True
    error: Optional[str] = None


# è§’è‰²é…ç½®å®šä¹‰
ROLE_CONFIGS: Dict[AIRole, RoleConfig] = {
    AIRole.REASONING: RoleConfig(
        role=AIRole.REASONING,
        description="è´Ÿè´£å¤æ‚æŽ¨ç†ã€é£Žé™©è¯„ä¼°ã€ç­–ç•¥å®¡æ ¸ã€‚ç”¨äºŽåˆ¤æ–­äº¤æ˜“ä¿¡å·æ˜¯å¦åˆç†æˆ–è§£è¯»å¼‚å¸¸çŽ°è±¡ã€‚",
        preferred_endpoints=["openai", "anthropic", "deepseek"],
        system_prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡åŒ–äº¤æ˜“ç­–ç•¥åˆ†æžå¸ˆï¼Œæ“…é•¿ï¼š
- å¤æ‚æŽ¨ç†å’Œé€»è¾‘åˆ†æž
- é£Žé™©è¯„ä¼°å’Œç­–ç•¥å®¡æ ¸
- è§£è¯»å¸‚åœºå¼‚å¸¸çŽ°è±¡
- åˆ¤æ–­äº¤æ˜“ä¿¡å·çš„åˆç†æ€§

è¯·ä»¥ç»“æž„åŒ–çš„æ–¹å¼è¾“å‡ºä½ çš„åˆ†æžç»“æžœï¼ŒåŒ…å«å†³ç­–å»ºè®®å’Œè¯¦ç»†è§£é‡Šã€‚""",
        default_max_tokens=1024,
        default_temperature=0.3,
        output_format="json"
    ),
    
    AIRole.WRITER: RoleConfig(
        role=AIRole.WRITER,
        description="è´Ÿè´£æ’°å†™äººç±»å¯è¯»çš„æŠ¥å‘Šã€æ—¥è®°ã€é‚®ä»¶ç­‰ã€‚å¼ºè°ƒå¯è¯»æ€§å’Œä¸­æ–‡èƒ½åŠ›ã€‚",
        preferred_endpoints=["deepseek", "openai", "anthropic"],
        system_prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èžæŠ¥å‘Šæ’°å†™ä¸“å®¶ï¼Œæ“…é•¿ï¼š
- æ’°å†™æ¸…æ™°ã€ä¸“ä¸šçš„ä¸­æ–‡æŠ¥å‘Š
- å°†å¤æ‚æ•°æ®è½¬åŒ–ä¸ºæ˜“è¯»çš„æ–‡å­—
- ä½¿ç”¨æ°å½“çš„æ ¼å¼ï¼ˆMarkdownï¼‰
- ä¿æŒä¸“ä¸šä½†æ˜“äºŽç†è§£çš„è¯­æ°”

è¯·ä»¥ Markdown æ ¼å¼è¾“å‡ºï¼Œä½¿ç”¨é€‚å½“çš„æ ‡é¢˜ã€åˆ—è¡¨å’Œå¼ºè°ƒã€‚""",
        default_max_tokens=2048,
        default_temperature=0.7,
        output_format="markdown"
    ),
    
    AIRole.MEMORY: RoleConfig(
        role=AIRole.MEMORY,
        description="è´Ÿè´£å¤„ç†é•¿ä¸Šä¸‹æ–‡æ‘˜è¦ã€åŽ†å²å›žé¡¾ã€‚é€‚ç”¨äºŽä¸ªäººè¡Œä¸ºåˆ†æžã€é•¿å‘¨æœŸè¡Œæƒ…è§£é‡Šã€‚",
        preferred_endpoints=["gemini", "anthropic", "openai"],
        system_prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº¤æ˜“è¡Œä¸ºåˆ†æžå¸ˆï¼Œæ“…é•¿ï¼š
- åˆ†æžé•¿æœŸäº¤æ˜“åŽ†å²å’Œè¡Œä¸ºæ¨¡å¼
- æ€»ç»“å…³é”®è¶‹åŠ¿å’Œè½¬æŠ˜ç‚¹
- è¯†åˆ«äº¤æ˜“ä¹ æƒ¯å’Œåå¥½
- æä¾›åŸºäºŽåŽ†å²çš„æ´žå¯Ÿ

è¯·æä¾›ç»“æž„åŒ–çš„æ‘˜è¦å’Œæ´žå¯Ÿã€‚""",
        default_max_tokens=2048,
        default_temperature=0.5,
        output_format="json"
    ),
    
    AIRole.RESEARCH: RoleConfig(
        role=AIRole.RESEARCH,
        description="è´Ÿè´£è”ç½‘æœç´¢å’Œæ‘˜è¦æ–°é—»ã€å…¬å‘Šã€é“¾ä¸Šæ•°æ®ã€‚",
        preferred_endpoints=["tavily", "groq", "openai"],
        system_prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸ç ”ç©¶åˆ†æžå¸ˆï¼Œæ“…é•¿ï¼š
- æœç´¢å’Œåˆ†æžå¸‚åœºæ–°é—»
- è§£è¯»å…¬å‘Šå’Œæ”¿ç­–å˜åŒ–
- åˆ†æžé“¾ä¸Šæ•°æ®å’Œè¶‹åŠ¿
- æä¾›ä¿¡æ¯æ¥æºå’Œå¯ä¿¡åº¦è¯„ä¼°

è¯·æä¾›å¸¦æœ‰æ¥æºé“¾æŽ¥çš„ç»“æž„åŒ–ä¿¡æ¯ã€‚""",
        default_max_tokens=1024,
        default_temperature=0.5,
        output_format="json"
    ),
    
    AIRole.CRITIC: RoleConfig(
        role=AIRole.CRITIC,
        description="è´Ÿè´£æŒ‘åˆºå’Œç”Ÿæˆåä¾‹ï¼Œä¾‹å¦‚å®¡æŸ¥äº¤æ˜“è®¡åˆ’ã€æŒ‘å‡ºæ½œåœ¨é£Žé™©ã€‚",
        preferred_endpoints=["anthropic", "openai", "deepseek"],
        system_prompt="""ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„äº¤æ˜“é£Žé™©å®¡æŸ¥å®˜ï¼ŒèŒè´£æ˜¯ï¼š
- å®¡æŸ¥äº¤æ˜“è®¡åˆ’å’Œç­–ç•¥
- è¯†åˆ«æ½œåœ¨é£Žé™©å’Œæ¼æ´ž
- æå‡ºåå¯¹æ„è§å’Œåä¾‹
- è¯„ä¼°é£Žé™©ç­‰çº§

è¯·ä»¥æ‰¹åˆ¤æ€§æ€ç»´åˆ†æžï¼ŒæŒ‡å‡ºæ‰€æœ‰å¯èƒ½çš„é—®é¢˜ã€‚è¾“å‡ºé£Žé™©ç­‰çº§ï¼ˆä½Ž/ä¸­/é«˜/æžé«˜ï¼‰å’Œå…·ä½“å»ºè®®ã€‚""",
        default_max_tokens=1024,
        default_temperature=0.4,
        output_format="json"
    ),
}


def get_role_config(role: Union[str, AIRole]) -> RoleConfig:
    """èŽ·å–è§’è‰²é…ç½®"""
    if isinstance(role, str):
        role = AIRole(role)
    return ROLE_CONFIGS.get(role)


def call_ai(
    role: Union[str, AIRole],
    prompt: str,
    *,
    context: Optional[Dict] = None,
    schema: Optional[Dict] = None,
    max_tokens: Optional[int] = None,
    temperature: Optional[float] = None,
    forced_endpoint: Optional[str] = None,
    system_override: Optional[str] = None,
) -> AIResponse:
    """
    æ ¹æ®è§’è‰²è°ƒç”¨é€‚å½“çš„ API ç«¯ç‚¹ã€‚è¿”å›žç»“æž„åŒ–æ•°æ®æˆ–æ–‡æœ¬ã€‚

    Args:
        role: è§’è‰²æ ‡è¯†ï¼Œå¦‚ ai_writer/ai_reasoningã€‚
        prompt: ä¸»æç¤ºè¯ã€‚
        context: å¯é€‰çš„é¢å¤–ä¸Šä¸‹æ–‡ï¼Œå¦‚åŽ†å²è®°å½•ã€è¡¨æ ¼ç­‰ã€‚
        schema: å¯é€‰çš„ JSON æ¨¡å¼ï¼Œè¦æ±‚æ¨¡åž‹è¾“å‡ºç¬¦åˆç»“æž„ã€‚
        max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦ã€‚
        temperature: éšæœºæ€§ã€‚
        forced_endpoint: æŒ‡å®š API ç«¯ç‚¹ï¼Œä¼˜å…ˆçº§é«˜äºŽé…ç½®ã€‚
        system_override: è¦†ç›–é»˜è®¤çš„ç³»ç»Ÿæç¤ºè¯ã€‚

    Returns:
        AIResponse: åŒ…å«æ¨¡åž‹è¿”å›žå†…å®¹å’Œæ‰€ç”¨ç«¯ç‚¹ä¿¡æ¯ã€‚
    """
    start_time = time.time()
    
    # èŽ·å–è§’è‰²é…ç½®
    if isinstance(role, str):
        try:
            role_enum = AIRole(role)
        except ValueError:
            return AIResponse(
                content="",
                endpoint="",
                role=role,
                latency=0,
                success=False,
                error=f"Unknown role: {role}"
            )
    else:
        role_enum = role
    
    config = get_role_config(role_enum)
    if not config:
        return AIResponse(
            content="",
            endpoint="",
            role=role_enum.value,
            latency=0,
            success=False,
            error=f"No config found for role: {role_enum.value}"
        )
    
    # æž„å»ºå®Œæ•´æç¤ºè¯
    system_prompt = system_override or config.system_prompt
    
    if context:
        context_str = json.dumps(context, ensure_ascii=False, indent=2)
        full_prompt = f"{prompt}\n\n### ä¸Šä¸‹æ–‡ä¿¡æ¯\n```json\n{context_str}\n```"
    else:
        full_prompt = prompt
    
    if schema:
        schema_str = json.dumps(schema, ensure_ascii=False, indent=2)
        full_prompt += f"\n\n### è¾“å‡ºæ ¼å¼è¦æ±‚\nè¯·æŒ‰ä»¥ä¸‹ JSON æ¨¡å¼è¾“å‡ºï¼š\n```json\n{schema_str}\n```"
    
    # è®¾ç½®å‚æ•°
    tokens = max_tokens or config.default_max_tokens
    temp = temperature if temperature is not None else config.default_temperature
    
    # èŽ·å– API ç®¡ç†å™¨
    api_manager = get_api_manager()
    
    # é€‰æ‹©ç«¯ç‚¹ç­–ç•¥
    if forced_endpoint:
        # å¼ºåˆ¶ä½¿ç”¨æŒ‡å®šç«¯ç‚¹
        endpoint = api_manager.get_endpoint(forced_endpoint)
        if not endpoint:
            return AIResponse(
                content="",
                endpoint=forced_endpoint,
                role=role_enum.value,
                latency=0,
                success=False,
                error=f"Endpoint not found: {forced_endpoint}"
            )
        endpoints_to_try = [endpoint]
    else:
        # æŒ‰ä¼˜å…ˆçº§èŽ·å–å¯ç”¨ç«¯ç‚¹
        endpoints_to_try = []
        for ep_name in config.preferred_endpoints:
            ep = api_manager.get_endpoint(ep_name)
            if ep and api_manager.is_endpoint_available(ep_name):
                endpoints_to_try.append(ep)
        
        # å¦‚æžœä¼˜å…ˆç«¯ç‚¹éƒ½ä¸å¯ç”¨ï¼ŒèŽ·å–ä»»æ„å¯ç”¨ç«¯ç‚¹
        if not endpoints_to_try:
            available = api_manager.get_available_endpoints()
            endpoints_to_try = available[:3]  # æœ€å¤šå°è¯•3ä¸ª
    
    if not endpoints_to_try:
        return AIResponse(
            content="",
            endpoint="",
            role=role_enum.value,
            latency=time.time() - start_time,
            success=False,
            error="No available API endpoints"
        )
    
    # å°è¯•è°ƒç”¨
    last_error = None
    for endpoint in endpoints_to_try:
        try:
            logger.info(f"[{role_enum.value}] Trying endpoint: {endpoint.name}")
            
            # è°ƒç”¨ API
            from orchestration.providers import OpenAICompatibleProvider
            
            provider = OpenAICompatibleProvider(
                name=endpoint.name,
                api_key=endpoint.api_key,
                base_url=endpoint.base_url,
                model=endpoint.model,
                timeout=endpoint.timeout
            )
            
            response = provider.generate(
                prompt=full_prompt,
                system=system_prompt,
                max_tokens=tokens,
                temperature=temp
            )
            
            if response.text and not response.text.startswith("["):
                # æˆåŠŸ
                latency = time.time() - start_time
                
                # å°è¯•è§£æž JSON
                parsed = None
                if config.output_format == "json":
                    try:
                        # å°è¯•ä»Žæ–‡æœ¬ä¸­æå– JSON
                        text = response.text
                        if "```json" in text:
                            text = text.split("```json")[1].split("```")[0]
                        elif "```" in text:
                            text = text.split("```")[1].split("```")[0]
                        parsed = json.loads(text.strip())
                    except:
                        pass
                
                # æ›´æ–°ç«¯ç‚¹ç»Ÿè®¡
                api_manager.record_success(endpoint.name, latency)
                
                return AIResponse(
                    content=response.text,
                    endpoint=endpoint.name,
                    role=role_enum.value,
                    latency=latency,
                    raw=response.raw,
                    parsed=parsed,
                    success=True
                )
            else:
                last_error = response.text
                api_manager.record_failure(endpoint.name)
                
        except Exception as e:
            last_error = str(e)
            logger.warning(f"[{role_enum.value}] Endpoint {endpoint.name} failed: {e}")
            api_manager.record_failure(endpoint.name)
            continue
    
    # æ‰€æœ‰ç«¯ç‚¹éƒ½å¤±è´¥
    return AIResponse(
        content="",
        endpoint="",
        role=role_enum.value,
        latency=time.time() - start_time,
        success=False,
        error=f"All endpoints failed. Last error: {last_error}"
    )


def call_ai_async(
    role: Union[str, AIRole],
    prompt: str,
    callback: Optional[Callable[[AIResponse], None]] = None,
    **kwargs
) -> str:
    """
    å¼‚æ­¥è°ƒç”¨ AIï¼Œè¿”å›žä»»åŠ¡ IDã€‚
    
    Args:
        role: è§’è‰²æ ‡è¯†
        prompt: æç¤ºè¯
        callback: å¯é€‰çš„å›žè°ƒå‡½æ•°
        **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™ call_ai
    
    Returns:
        str: ä»»åŠ¡ ID
    """
    from cloud.enhanced_publisher import EnhancedCloudTaskPublisher, TaskPriority
    
    publisher = EnhancedCloudTaskPublisher()
    
    task = publisher.publish(
        name=f"ai_call_{role}",
        payload={
            "role": role if isinstance(role, str) else role.value,
            "prompt": prompt,
            "kwargs": kwargs
        },
        priority=TaskPriority.NORMAL.value,
        timeout=120.0,
        tags=["ai_call", role if isinstance(role, str) else role.value]
    )
    
    return task.task_id


# ä¾¿æ·å‡½æ•°
def reason(prompt: str, context: Optional[Dict] = None, **kwargs) -> AIResponse:
    """è°ƒç”¨æŽ¨ç†è§’è‰²"""
    return call_ai(AIRole.REASONING, prompt, context=context, **kwargs)


def write(prompt: str, context: Optional[Dict] = None, **kwargs) -> AIResponse:
    """è°ƒç”¨å†™ä½œè§’è‰²"""
    return call_ai(AIRole.WRITER, prompt, context=context, **kwargs)


def remember(prompt: str, documents: Optional[List[str]] = None, **kwargs) -> AIResponse:
    """è°ƒç”¨è®°å¿†è§’è‰²"""
    context = {"documents": documents} if documents else None
    return call_ai(AIRole.MEMORY, prompt, context=context, **kwargs)


def research(query: str, num_sources: int = 5, **kwargs) -> AIResponse:
    """è°ƒç”¨ç ”ç©¶è§’è‰²"""
    context = {"num_sources": num_sources}
    return call_ai(AIRole.RESEARCH, query, context=context, **kwargs)


def critique(plan: str, criteria: Optional[List[str]] = None, **kwargs) -> AIResponse:
    """è°ƒç”¨æ‰¹è¯„è§’è‰²"""
    context = {"criteria": criteria} if criteria else None
    return call_ai(AIRole.CRITIC, plan, context=context, **kwargs)


# å¯¼å‡º
__all__ = [
    "AIRole",
    "RoleConfig",
    "AIResponse",
    "ROLE_CONFIGS",
    "get_role_config",
    "call_ai",
    "call_ai_async",
    "reason",
    "write",
    "remember",
    "research",
    "critique",
]
